\documentclass[letterpaper,12pt]{article}
\pagestyle{empty}

\pdfpagewidth 8.5in
\pdfpageheight 11in

\hoffset = 0pt
\oddsidemargin = 0pt
\marginparwidth = 0pt

\voffset = 0pt
\topmargin = 0pt
\headheight = 0pt
\headsep = 12pt

\textwidth = 6.5in
\textheight = 9in
\footskip = 0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}

\author{Erik Keever}
\title{Imogen User's Manual}

\begin{document} 

\maketitle

\section{Setup}

This section will guide you through the Imogen acquisition and setup process.

The setup process consists in making sure that you have the proper libraries
available and informing Imogen of where they are through the Make.config file
in the top level directory. The template Make.config.default should be copied
to Make.config and then setup per instructions below.

\subsection{Download}

Imogen lives online at \url{https://github.com/imogenproject/gpuImogen}

To download it into directory \textit{./gpuimogen} from the public repo,

\textit{ git clone https://github.com/imogenproject/gpuImogen.git ./gpuimogen }

\subsection{Matlab}

Imogen runs in Matlab and so obviously Matlab is a prerequisite to have. A standard
Matlab installation will have everything you need - the parallel computing toolkit
is not needed.

The \textit{MATLAB\_DIR} variable in Make.config must be set to the root of
the Matlab installation (e.g. /opt/Matlab/R2013b). Running Imogen in a different
Matlab version than used here to compile the binary modules is less likely than
might be imagined to cause problems.

\subsection{MPI}

MPI, even if parallel processing is not used, must be available (Imogen still
uses MPI to check that it is running serially), including development headers
to compile the Matlab mex files.

In Make.config, \textit{MPI\_DIR} must refer to the directory containing the include/ folder
that holds the mpi.h file, i.e. the --prefix specified when MPI was installed.

\subsection{CUDA}

Imogen's GPU routines are written in CUDA and access to cuda 4.0 or later
libraries is required. The runtime alone isn't sufficient, the SDK is
needed to provide the headers.

Once it is installed set \textit{CUDA\_DIR} to the install location.

Other variables to be set are
\begin{itemize}
\item \textit{CUDA\_LDIR} - either 'lib' on 32bit or 'lib64' on 64bit machines
\item \textit{NVARCH} - \textit{compute\_20} for Fermi, \textit{compute\_30} for Kepler
\item \textit{NVCODE} - \textit{sm\_20} for Fermi, \textit{sm\_30} for Kepler
\end{itemize}

NVARCH and NVCODE are further explained by the nvcc manual page.

\subsection{Compiling}

Once all the above variables are set, run \textit{make} in both the mpi/ and gpuclass/
directories. Both build in parallel so feel free to use -j. 

\subsection{Filesystem}

Imogen will open/create the directory ~/Results. It is not required that this
be a shared filesystem.

\subsection{Cluster access}

Imogen invokes itself in parallel with the './imogen cluster ...' format.

This takes place on lines 95 to 124 of the run/imogen file. Before running
in parallel on a cluster it would be a good idea to examine this section since
it will almost certainly need to be amended, if only to set the package
names to values appropriate to your cluster system.

\subsection{Troubleshooting}

It is very important that PGW, MPI and the CUDA modules all be compiled
using the same compiler. PGW in particular is insistent about the version
of libfortran it accesses.

Unfortunately this is growing progressively more difficult as CUDA requires
newer GCCs and Mathworks' MEX compiler-wrapper lags.

If Imogen fails because Matlab complains about being unable to resolve an MPI
symbol, the problem is that Matlab defaults to lazy symbol resolution when
loading dynamic libraries and apparently something about the MPI lib trips
up on this. The solution:
\begin{itemize} \item LD\_PRELOAD="\$MPIDIR/lib/libmpi.so"
\end{itemize}
Putting this in your .bashrc (or as sysadmin, in the system's /etc/profile.d) will
make it go away permanently.

\section{Invocation}

\subsection{Standalone / batch}

Imogen is started from console to run by itself through the script run/imogen

There are three separate general types of run, all of which invoke "matlab -r":
\begin{itemize}
\item serial - One process uses the given set of GPUs; 'imogen' calls Matlab.
\item parallel - N processes use the given set of GPUs; 'imogen' calls mpirun calls Matlab
\item cluster - 'imogen' submits a script calling itself to qsub
\end{itemize}

Serial and parallel are for small size simulations that work on a single system,
or perhaps for testing on a local cluster with an MPI hostfile.
The cluster option will be necessary to interface with batch schedulers on large machines.

The invocation syntaxes (also available through ./imogen -help) are as follows:
\begin{itemize}
\item ./imogen serial runfile.m [stream\# [GPU\#]];
\item ./imogen runfile.m; A shortcut that assumes serial, stream 0 and GPU 0.
\item ./imogen parallel runfile.m [stream\#] [NP]
\end{itemize}

\section{Selftest}

\textbf{This is not yet complete -- Erik}

Imogen comes with two test suites, \begin{tt}run\_tortureTest\end{tt} and \begin{tt}run\_FullTestSuite\end{tt}.

The torture test runs a series of unit tests that confirm that all the basics are working, and
then fuzz most of the assistant routines a programmable number of times with random data and
resolutions; This is effective at revealing size-dependent, data-dependent and intermittent bugs.
Depending on the number of fuzz tests run and their size, it may take a number of minutes
to finish executing.

The test suite ``simulation'' is a large runfile which performs grid refinement tests on a large
number of problems in one, two and three dimensions. Ultimately it should issue a report
that all convergence rates are asymptotically as expected; If all tests are run with a high
degree of refinement, this suite may be expected to take hours to execute.

\section{Physics solved by the Imogen simulation engine}

\subsection{Full Equation Set}

The broadest writing of the equations Imogen is capable of solving:
\begin{align*}
\frac{\partial \rho}{\partial t} &+ \nabla (\rho v) &= 0 & & \\
\frac{\partial \rho \vec{v}}{\partial t} &+ \nabla_i \left( \rho \vec{v} v_i + \mathbb{I} P \right) &= 
- \textcolor{red}{\rho \nabla \phi_g } & & - 
\textcolor{blue}{ \sum_k n_{d,k} F_{drag,k}  }\\
\frac{\partial E_{tot}}{\partial t} &+ \nabla v (E_{tot} + P) &= 
\textcolor{red}{\rho v \cdot \nabla \phi_g} & 
- \textcolor{green}{\Gamma_0 \rho^2 T^\theta } &
- \textcolor{blue}{\sum_k n_{d,k} v_{d,k} \cdot F_{drag,k}} \\
\frac{\partial \rho_{d,k}}{\partial t} &+ \nabla (\rho_{d,k} v_{d,k}) &= 0 & & \\
\frac{\partial \rho_{d,k} v_{d,k}}{\partial t} &+ \nabla (\rho_{d,k} v_{d,k} v_{d,k}) &= n_{d,k} F_{drag,k}  & & \\
\frac{\partial T_{d,k}}{\partial_t} &+ \nabla (v_{d,k} T_{d,k} ) &= & & heat+radi
\end{align*}
Where here $k$ denotes an arbitrary number of dust fluids.

With auxiliary equations
\begin{align*}
\textcolor{red}{\nabla^2 \phi_{self} = 4 \pi G \rho} \\
\textcolor{red}{\phi_g = \phi_{self} + \phi_{static} + \sum_{i} \frac{-G M_i}{|x - x_i|} } \\
\textcolor{blue}{F_{drag} = \frac{s_0^2 F_{epstein} + s^2 F_{stokes}}{s_0^2 + s^2}} \\
\textcolor{blue}{F_{epstein} \approx \frac{-4 \pi}{3} \rho_{gas} s^2 \sqrt{\frac{8}{\gamma \pi}} \sqrt{c_s^2 + 9 \pi |dv|^2 / 128}  \vec{dv} } \\
\textcolor{blue}{F_{stokes} = -.5 C_d \pi s^2 \rho_{gas} |dv| \vec{dv} }
\end{align*}
In the above, G is the gravitational coupling constant and $\phi_{static}$ represents a stationary
potential field defined at simulation start time.
$F_{drag}$ generates a smooth interpolation between the rarefied gasdynamics regime experienced
by a particle much smaller than the mean free path, $F_{epstein}$, and the viscous drag 
on a particle in the continuum limit given by $F_{stokes}$.
$F_{epstein}$ as given represents the approximation
given by Kwok (1975) for the exact Epstein drag, due to the high cost of evaluating multiple
transcendental functions occurring in it.


The Euler equations, unadorned, are self-similar and lack any characteristic scale. Activating
any of the non-ideal physics in Imogen introduces scales and requires choosing appropriate scaling
constants.

Conservative transport terms are in black. Gravitation related terms are in red.
Optically thin radiation terms are in green. Dust terms are in blue.

Self gravity, stationary potential, and compact objects are enableable separately.
Activating any of them requires choosing a gravitational coupling constant G.

Activating radiative emission requires choosing a radiation scaling constant $\Gamma_0$.
The code computes as written, using the square of \textit{mass density}. Because we assume an
ideal gas law, we are actually considering
\begin{align*}
\Gamma' n^2 (P/\rho)^{\theta}
\end{align*}
with $\Gamma' = \Gamma_0 \mu^{\theta-2} K_b^{-\theta}$.

Imogen contains the following unitful objects which must be defined if the relevant
physics are in use.
\begin{itemize}
\item G - Gravitational coupling constant
\item $K_b$ - Boltzmann constant
\item $\mu$ - Gas mean molecular weight
\item $m_k$ - Mass of dust particles of type k
\item $s_k$ - Size of dust particles of type k
\end{itemize}

There are four fundamental unitful quantities in Imogen: mass, length, time, and temperature.

As an example of how they might be chosen: Kojima disks are defined with G=1 and M* = 1. The
orbital speed at a radius of 1 is defined as 1. By using $p = \rho^\gamma$ as the equation
of state, implicitly $k_b/\mu$ is chosen.



\subsection{CFD}

At its core Imogen solves the Euler equations, written here in conservative form:
\begin{align*}
\frac{\partial \rho}{\partial t} &+ \frac{\partial}{\partial x_i} \rho v_i &= 0 \\
\frac{\partial (\rho v_i)}{\partial t} &+ \frac{\partial}{\partial x_j} (T_{ij} + \mathbb{I} P) &= 0 \\
\frac{\partial E}{\partial t} &+ \frac{\partial}{\partial x_i} v_i (E + P) &= 0
\end{align*}
with mass density $\rho$, fluid bulk velocity $v_i$, momentum stress tensor
$T_{ij} = \rho v_i v_j$, identity tensor $\mathbb{I}$, scalar thermal pressure $P$,
and total energy density $E = \frac{1}{2} \rho v^2 + \epsilon$.

These are closed by the equation of state $P(\epsilon)$. Imogen implements the
adiabatic ideal gas equation of state, $P = (\gamma-1)\epsilon$ for $\gamma > 1$
(physical gas values are $1 < \gamma \le 5/3$).

With the assumption of a gamma law, the energy flux is written into the
code as $v_i \times (\frac{1}{2} \rho v^2 + \frac{\gamma}{\gamma-1}P)$ with $P$ calculated
before fluxing.

\subsubsection{Operator splitting}

Imogen uses a dimensionally split flux solving scheme. Assuming a splittable Hamiltonian
exists in the form
\[\mathbf{H} = \mathbf{E} + \mathbf{F} + \mathbf{G}\]
where in our case, $E$, $F$, and $G$ represent the fluid fluxes in 3 non-degenerate
directions (normally taken as 3 orthogonal directions in a curvilinear coordinate
system: Here, fluxes in the x, y, and z directions), an approximate scheme of the form
\[e^{\epsilon \mathbf{H}} = e^{\epsilon \mathbf{E}} e^{\epsilon \mathbf{F}}
e^{\epsilon \mathbf{G}} + \mathbb{O}(\epsilon^2) \]
always exists, and the reader may easily verify that the local error associated with
doing this with two operators equals their commutator times $\epsilon^2$.

Strang [reference!] showed that by performing such a first order step, then
applying the same operators in exactly reversed order, will cancel all the first-order
errors. This Strang splitting provides an easy way of constructing second spatial order
solvers.

Denoting the operators that solve the inviscid flux equations in the three directions as
$\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{Z}$, there are 6 ways to order them which are the
elements of the permutation group of $\{ \mathbf{X}, \mathbf{Y}, \mathbf{Z} \}$.
In a two-dimensional simulation, there are only two orderings ($XYYX$ and $YXXY$).

Imogen cycles through them iteration to iteration, the idea being that this will average
away (or at least partly so) any systematic errors associated with a given ordering.

There are additional operators associated with MHD (magnetic fluxing operators), non-inertial
frames, radiation, and scalar potental fields (fixed potentials, point objects or
self-gravitation).

\textbf{The MHD algorithm has major problems with low-beta plasma and seems to be
subtly broken at present.}

\subsection{MHD}

Imogen can evolve magnetic fields in the limit of Ideal MHD where the Euler equations
become
\begin{align*}
\frac{\partial \rho}{\partial t} &= -\frac{\partial}{\partial x_i} \rho v_i \\
\frac{\partial (\rho v_i)}{\partial t} &= -\frac{\partial}{\partial x_j} (T_{ij} + \mathbb{I}(P+B^2/2) \\
\frac{\partial E}{\partial t} &= -\frac{\partial}{\partial x_i} v_i (E + P) - B_i (v_j B_j) \\
\frac{\partial B_i}{\partial t} &= \frac{\partial}{\partial x_i} (v_i B_j - B_i v_j)
\end{align*}
now with the stress tensor $T_{ij} = \rho v_i v_j - B_i B_j$ and under the requirement
$\nabla \mathbf{B} = 0$.

There is no computational requirement that the \textbf{B} field be initialized this way,
however Imogen solves no advection equation for $\mathbf{B}$-charge and any divergence
introduced will simply stay there.

\subsection{Frame rotation}

In many cases a simulation is desired of an object that exhibits coherent rotation at
high speeds.

Imogen has the Eulerian explicit CFL constraint is set by
\[ dt < dx (|v_i| + c_s) \]
and so if $v_i$ can be significantly reduced by setting the grid to co-rotate with a flow,
the timestep can be significantly increased for advection-dominated flows.

Frame rotation introduces the following additional source terms:
\begin{align*}
\partial_t \rho &= 0 \\
\partial_t v &= 2 \omega_z \times \vec{r} + \hat{r} \omega^2 \\
\partial_t \epsilon_{int} &= 0
\end{align*}
i.e. the coriolis and centripetal terms are implemented as fictitous forces.

Imogen implements this term so as to exactly preserve internal energy (the total
energy density is updated as exactly the change in KE density)

Imogen checks for a rotating frame at simulation initialization and performs the
transformation to the noninertial frame for you - \textbf{simulations should be initialized
in the lab inertial frame.}

\subsection{Radiation}

Imogen can solve optically thin radiation with power law cooling,

(power law lambda)

Programming in different cooling laws is extremely simple as a test by making additional
entries to the experiment/Radiation.m file, templated after the thin power law radiation
one. Of course, don't expect them to be as fast as the GPU accelerated routine.

While the power law cooling is reliable enough in and of itself, it has exhibited a
remarkable ability to derange Imogen's fluid solver routines. None of them has exhibited
the ability to handle an extremely slowly-moving radiating shock front (i.e. the motion
associated with the finite accuracy of the simulation) without some measure of oscillatory
density error at the adiabatic jump.

This is related to a generic problem associated with nearly stationary shocks in 
shock-capturing schemes. Without radiation, HLL will exhibit bounded oscillation of the
measured shock position of a plane shock vs the exact result. However radiation adds
an additional feedback loop which greatly increases the ``desire'' of the shock to
place itself exactly at a cell boundary.

(Post pics of evil radiative shocks here)

Work by Roe (REF) has indicated a solution to the instability problem in 1D,
however the extension to multiple dimensions is as yet unclear except perhaps conceptually.

\subsection{Gravity}

\subsubsection{Stationary gravity field}

An arbitrary stationary potential field may be established. This may e.g. represent
a fixed star, or may be used in a Cowling approximation to examine NSG modes of a
self-gravitating equilibrium (NOTE: Still can't solve gravity to generate the equilibrium...)

\subsubsection{Point gravity}

Imogen allows the addition of massive points to the fluid simulation which obey Newton's
laws. Both point-point and point-fluid gravitational force are computed.

These points, ``compact objects'' as Imogen calls them, are used as standins for stars
or planets which cannot be resolved at reasonably available levels of resolution.

Far away from the object, they behave as point masses. Within a distance defined as
its 'radius', matter is considered as having been accreted. Each step, the mass and
linear and angular momentum in cells whose center is within r of a compact mass
are added to the compact object and replaced with grid's vaccuum values instead.

NOTE: This is not the desired behavior in some cases - e.g. a planet would require a surface
pressure, not a surface vaccuum, in order to be embedded in the disk.

\textbf{Point does not feel fluid gravity presently.}

\subsubsection{Self gravity}

This is basically dead for now, especially in parallel.

The serial solver could easily enough be resurrected but there's no way I'm going
to have time to setup a parallel Poisson solver, let alone shovel it onto the GPU.

\section{Physics solvers in the initializers}

Many fluid and physics subproblems come up when writing initial condition generators
for Imogen. Some of these have convenient self-contained solvers available within
Imogen.

The available units include:
\begin{itemize}
\item \begin{tt}J = HDJumpSolver(Mach, $\gamma$, $\theta$)\end{tt}: Returns a structure $J$ describing the
only physical shock solution to a hydrodynamic shock of strength $Mach$ in a fluid of
adiabatic index $\gamma$ and preshock shock-normal flow inclination $\theta$ (in degrees).
\item \begin{tt}J = MHDJumpSolver(Mach, Alfven Mach, $\gamma$, $\theta$)\end{tt}: Solves the MHD jump
equations for the slowest shock permitted.
\item \begin{tt}F = fim(X, @f(X))\end{tt}: Short for Fourier Integral Method, calculates the integral of
anonymous function f on the presumed regularly spaced points $X$ by calculating a polynomial
that renders $f$ 3 times continuous at the endpoints, integrating the polynomial directly
and the residual by Fourier transform.
\item \begin{tt}The ICM\end{tt}: \begin{tt}icm.m\end{tt} is currently very much a prototype and not ready for normal use.
It is short for Integral Constraints Method.
\item \begin{tt}t = pade(c\_n, a, b, x)\end{tt}: Given a polynomial $\sum_n c_n x^n$, calculates the (a,b)
Pade approximant P(a)/Q(b) of the polynomial and evaluates it at x, for any $a+b \le 6$.
\item RadiatingFlowSolver class: Solves the equations of a power-law-radiating hydrodynamic
or magnetohydrodynamic flow. It initializes itself using the 4th (MHD) or 6th (HD) order power
series solution of the flow and then integrates using a 6th order implicit LMM, restarting with
reduced step size when it encounters sharp flow features (knees).
\item Linear wave eigenvectors: The function eigenvectorEntropy, eigenvectorAlfven, eigenvectorSonic,
eigenevectorMA(rho, csq, v, b, k, wavetype) return the linear wave eigenvectors associated with the
flows with the given uniform density, thermal acoustic soundspeed, velocity, magnetic field and wavevector.
Alfven and sonic waves have wavetype +1 for a wave which advances towards $\mathbf{k}$ and -1 for
against $\mathbf{k}$. The MA wavetype can have either sign, with magnitude 2 for a fast MS
wave and magnitude 1 for a slow MS wave. If a nonzero B is passed to a sonic wave, it will invoke the
fast MS solver (as the fast wave is the one connected with the nonmagnetized sonic wave).
\item The bow shock problem, in two-dimensional testing of a source with outflow, has acquired a routine
analyticalOutflow() which solves the exact adiabatic expansion of a $\gamma=5/3$ gas away from a cylinder
with the inner boundary condition of $v_{radial} = v_0$ at $r=r_0$.
\end{itemize}

\section{Programming Experiments}

\subsection{Common parameters of all experiments}

All experiment initializers should inherit from the \begin{tt}Initializer\end{tt} class
and share its not inconsiderable breadth of parameters. Among the more important 
names are
\begin{itemize}
\item \begin{tt}cfl\end{tt} coefficient by which the CFL-limit timestep is scaled. \textbf{WARNING
IMOGEN DOES NOT SANITY CHECK THIS}
\item \begin{tt}gamma\end{tt} - The adiabatic index of the fluid
\item \begin{tt}grid\end{tt} - The [nx ny nz] grid resolution of the \textit{global} domain. This
should normally be set when creating the initializer and not messed with again.
\item \begin{tt}iterMax\end{tt} - the maximum number of timesteps to take
\item \begin{tt}useInSituAnalysis\end{tt} - If nonzero, Imogen uses the given in-situ analysis tool
\item \begin{tt}inSituHandle\end{tt} - @AnalyzerClass which must meet the requirements of an
in-situ analyzer (see REF)
\item \begin{tt}stepsPerInSitu\end{tt} - Number of timesteps between calls to the analyzer
\item \begin{tt}frameRotateOmega\end{tt} - If nonzero, Imogen places the simulation into a rotating
frame and applies appropriate source terms. \textbf{Simulation is still initialized in lab frame}
\item \begin{tt}frameRotateCenter\end{tt} - If frame is rotating, this is the center (\textbf{in cells})
about which the frame rotates on the $(x,y)$ plane.
\end{itemize}

\subsection{How to setup frame rotation}

To turn on the rotating frame during start, simply set a nonzero value for the \begin{tt}frameRotateOmega\end{tt}
parameter, and assign a 2-element array $\left[x_0, y_0\right]$ to \begin{tt}frameRotateCenter\end{tt}
and Imogen will automatically transform the fluid momentum field into the noninertial frame at start.

Caution: $x_0$ and $y_0$ need to be cell indices giving the origin.

If the simulation is in cylindrical coordinates, only rotation about the origin axis is supported.
Any rotation origin other than \begin{tt}[0, 0]\end{tt} will cause a warning.

The rotation rate can be altered during simulation time by calling
\begin{tt}
alterFrameRotation(run, mass,\newline
ener, mom, newOmega)
\end{tt}
in the user's in-situ analysis function. This has no physical effect (outside truncation error)
and will not e.g. generate the Euler term, it simply alters the rate at which the grid traverses
the fluid in the lab frame. Note also that the alterFrameRotation function is not GPU accelerated
and should thus be called rarely.

It is not acceptable to simply alter \begin{tt}run.frameRotateOmega\end{tt} during the run.
This will alter the behavior of the sourcing function, however it will NOT update the
\textit{frame-dependent} momentum and kinetic energy densities.

\subsection{How to setup radiation}

\subsection{How to setup a potential field}

\subsection{How to set up a compact object}

\subsection{How to setup in-situ analysis}

Recognizing that it is not practical to save the entire simulation output for post-hoc analysis in
most cases, Imogen allows the construction of in-situ analyzers. These are classes that must have
two public members:
\begin{itemize}
\item \begin{tt}.FrameAnalyzer(mass, ener, mom, mag, run)\end{tt}
\item \begin{tt}.finish(run)\end{tt}
\end{itemize}

They must be set at initialization time with these three parameters in the initializer:
\begin{itemize}
\item \begin{tt}useInSituAnalysis\end{tt} - Set to nonzero to enable
\item \begin{tt}inSituHandle\end{tt} - @Class which must meet the requirements of an
in-situ analyzer (see above)
\item \begin{tt}stepsPerInSitu\end{tt} - Number of timesteps between calls to the .FrameAnalyzer() function
\end{itemize}

Imogen calls the \begin{tt}.finish\end{tt} method after exiting the CFD iteration loop. At this point the
ISA needs to write to disk any data not already written because \begin{tt}imogen()\end{tt} is about to 
return.

\section{The RealtimePlotter}

The RealtimePlotter is a peripheral that does just what its name suggests, and throws up a pretty little
GUI window to control the plotting process.



\section{Experiments}

\input{../experiment/Advection/advectionDoc.tex}
\input{../experiment/BowShock/bowDoc.tex}
\input{../experiment/CentrifugeTest/centrifugeDoc.tex}
\input{../experiment/DoubleBlast/doubleblastDoc.tex}
\input{../experiment/RiemannProblem/einfeldtDoc.tex}
\input{../experiment/HachisuDisk/hachisuDoc.tex}
\input{../experiment/Implosion/implosionDoc.tex}
\input{../experiment/Jet/jetDoc.tex}
\input{../experiment/KelvinHelmholtz/khDoc.tex}
\input{../experiment/RadiatingShock/radiativeShockDoc.tex}
\input{../experiment/RayleighTaylor/RT_Doc.tex}
\input{../experiment/RichtmyerMeshkov/richtmyermeshkovDoc.tex}
\input{../experiment/SedovTaylorBlastWave/sedovDoc.tex}
\input{../experiment/RiemannProblem/sodDoc.tex}
\input{../experiment/ShuOsherTube/shuoshertubeDoc.tex}

\end{document}
