\documentclass[letterpaper,12pt]{article}
\pagestyle{empty}

\pdfpagewidth 8.5in
\pdfpageheight 11in

\hoffset = 0pt
\oddsidemargin = 0pt
\marginparwidth = 0pt

\voffset = 0pt
\topmargin = 0pt
\headheight = 0pt
\headsep = 12pt

\textwidth = 6.5in
\textheight = 9in
\footskip = 0pt

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{color}

% seriously?
\definecolor{orange}{rgb}{1.0, .5, 0}

\author{Erik Keever}
\title{Imogen User's Manual}

\begin{document} 

\maketitle

\section{Setup}

This section will guide you through the Imogen acquisition and setup process.

The setup process consists in making sure that you have the proper libraries
available and informing Imogen of where they are through the Make.config file
in the top level directory. The template Make.config.default should be copied
to Make.config and then setup per instructions below.

\subsection{Download}

Imogen lives online at \url{https://github.com/imogenproject/gpuImogen}

To download it into directory \textit{./gpuimogen} from the public repo,

\textit{ git clone https://github.com/imogenproject/gpuImogen.git ./gpuimogen }

\subsection{Matlab}

Imogen runs in Matlab and so obviously Matlab is a prerequisite to have. A standard
Matlab installation will have everything you need - the parallel computing toolkit
is not needed (because why would pay THAT much for a wrapper around MPI?)

The \textit{MATLAB\_DIR} variable in Make.config must be set to the root of
the Matlab installation (e.g. /opt/Matlab/R2017b). It is generally possible to use
a \textit{later} Matlab version with modules compiled using a previous version
(forward compartibility) however the reverse is not true (lack of reverse compatibility)
and will likely result in crashes caused by unresolvable symbols.

\subsection{MPI}

MPI, even if parallel processing is not used, must be available (Imogen still
uses MPI to check that it is running serially), including development headers
to compile the Matlab mex files.

In Make.config, \textit{MPI\_DIR} must refer to the directory containing the include/ folder
that holds the mpi.h file, i.e. the --prefix specified when MPI was installed.

\subsection{CUDA}

Imogen's GPU routines are written in CUDA and access to cuda 4.0 or later
libraries is required. The runtime alone isn't sufficient, the SDK is
needed to provide the headers.

Once it is installed set \textit{CUDA\_DIR} to the install location.

Other variables to be set are
\begin{itemize}
\item \textit{CUDA\_LDIR} - either 'lib' on 32bit or 'lib64' on 64bit machines
\item \textit{NVARCH} - \textit{compute\_20} for Fermi, \textit{compute\_30} for Kepler
\item \textit{NVCODE} - \textit{sm\_20} for Fermi, \textit{sm\_30} for Kepler
\end{itemize}

NVARCH and NVCODE are further explained by the nvcc manual page.

\subsection{Compiling, linking and paths}

Once all the above variables are set, run \textit{make} in first the mpi/ and then the gpuclass/
directories. Both build in parallel so feel free to use -j to compile faster. mpi/
must be built first because GPU routines link to MPI ones.

\textbf{WARNING}: This compilation process involves two compilers (for C and for CUDA)
and two wrappers (nVidia's NVCC and Matlab's MEX). All of the following conditions
must be true for compilation to succeed:
\begin{itemize}
\item The compiler used to compile the MPI,
\item Must be the compiler used by Matlab's MEX (\textbf{mex -setup is mandatory}, loading the module or putting it first in \$PATH is not enough)
\item Must be the host compiler used by NVCC (first in \$PATH is sufficient)
\end{itemize}

There are no particular additional constraints for execution:
\begin{itemize}
\item In general, the environment which successfully builds will work for execution. Note the loaded modules at build time!
\item This includes the compiler used (An older libstdc++ will likely cause unresolved symbol errors)
\item Executing CUDA toolkit runtime must be at least as new as compilation CUDA toolkit
\item All nodes must use the exact same MPI version and compiler library (i.e. libstdc++) the code was built with, or abandon all hope ye who enter
\end{itemize}


\subsection{Filesystem}

Imogen will open/create the directory ~/Results. This must be a shared filesystem.
The code uses rank zero for file creation, followed by all ranks waiting for a
consistent filesystem view (meaning all ranks see the exact same files and sizes)

\subsection{Cluster access}

Imogen invokes itself in parallel with the './imogen cluster ...' format.

This takes place on lines 95 to 124 of the run/imogen file. Before running
in parallel on a cluster it would be a good idea to examine this section since
it will almost certainly need to be amended, if only to set the package
names to values appropriate to your cluster system.

\subsection{Troubleshooting}

It is very important that MPI and the CUDA modules all be compiled
using the same compiler.

If Imogen fails because Matlab complains about being unable to resolve an MPI
symbol, the problem is that Matlab defaults to lazy symbol resolution when
loading dynamic libraries and apparently something about the MPI lib trips
up on this. The solution:
\begin{itemize} \item LD\_PRELOAD="\$MPIDIR/lib/libmpi.so"
\end{itemize}
Putting this in your .bashrc (or as sysadmin, in the system's /etc/profile.d) will
make it go away permanently.

\section{Invocation}

\subsection{Standalone / batch}

Imogen is started from console to run by itself through the script run/imogen

There are three separate general types of run, all of which invoke "matlab -r":
\begin{itemize}
\item serial - One process uses the given set of GPUs; 'imogen' calls Matlab.
\item parallel - N processes use the given set of GPUs; 'imogen' calls mpirun calls Matlab
\item cluster - 'imogen' submits a script calling itself to qsub
\item talapas - specialized cluster invoke aimed at running on UO RACS system
\end{itemize}

Serial and parallel are for small size simulations that work on a single system,
or perhaps for testing on a local cluster with an MPI hostfile.
The cluster option will be necessary to interface with batch schedulers on large machines.

The invocation syntaxes (also available through ./imogen -help) are as follows:
\begin{itemize}
\item ./imogen serial runfile.m [stream\# [GPU\#]];
\item ./imogen runfile.m; A shortcut that assumes serial, stream 0 and GPU 0.
\item ./imogen parallel runfile.m [stream\#] [NP]
\end{itemize}

The magic number -1 for the set of GPUs is for when the code is invoked using the SLURM 
scheduler with SLURM GPU control. This instructs the code the code to determine its GPUs
by doing \begin{tt}getenv('GPU\_DEVICE\_ORDINAL')\end{tt}.

\section{Selftest}

\begin{textbf}This is not yet complete -- Erik \end{textbf}

Imogen comes with two test suites, \begin{tt}run\_tortureTest\end{tt} and \begin{tt}run\_FullTestSuite\end{tt}.

The torture test runs a series of unit tests that confirm that all the basics are working, and
then fuzz most of the assistant routines a programmable number of times with random data and
resolutions; This is effective at revealing size-dependent, data-dependent and intermittent bugs.
Depending on the number of fuzz tests run and their size, it may take a number of minutes
to finish executing.

The test suite ``simulation'' is a large runfile which performs grid refinement tests on a large
number of problems in one, two and three dimensions. Ultimately it should issue a report
that all convergence rates are asymptotically as expected; If all tests are run with a high
degree of refinement, this suite may be expected to take hours to execute.

\section{Physics solved by the Imogen simulation engine}

\subsection{Full Equation Set}
% red - conservative ideal transport
% orn - gravitational
% grn - radiative
% blu - gas/dust
% vio - geometric
% cya - frame rotation
% mag - magnetic?

The broadest writing of the equations Imogen is capable of solving:
\begin{align*}
\textcolor{red}{\frac{\partial \rho}{\partial t}} & \textcolor{red}{ + \nabla (\rho v)} &= 0 & &  \\
\textcolor{red}{\frac{\partial \rho \vec{v}}{\partial t} } & \textcolor{red}{+ \nabla_i \left( \rho \vec{v} v_i + \mathbb{I} P \right)} &= 
- \textcolor{orange}{\rho \nabla \phi_g } & & - 
\textcolor{blue}{ \sum_k n_{d,k} F_{drag,k}  }\\
\frac{\partial E_{tot}}{\partial t} &+ \nabla v (E_{tot} + P) &= 
\textcolor{orange}{\rho v \cdot \nabla \phi_g} & 
- \textcolor{green}{\Gamma_0 \rho^2 T^\theta } &
- \textcolor{blue}{\sum_k n_{d,k} v_{d,k} \cdot F_{drag,k}} \\
\textcolor{red}{\frac{\partial \rho_{d,k}}{\partial t}} & \textcolor{red}{+ \nabla (\rho_{d,k} v_{d,k})} &= 0 & & \\
\textcolor{red}{\frac{\partial \rho_{d,k} v_{d,k}}{\partial t}} &+ \textcolor{red}{\nabla (\rho_{d,k} \vec{v}_{d,k} v_{d,k})}
&= \textcolor{blue}{n_{d,k} F_{drag,k}}  & & \\
\frac{\partial T_{d,k}}{\partial_t} &+ \nabla (v_{d,k} T_{d,k} ) &= & & heat+radi
\end{align*}
Where here $k$ denotes an arbitrary number of dust `fluids.' Currently the number of fluids is restricted to one.

\subsection{Auxiliary equations for closure, gravitation and dust drag}

The transport equations are given closure by the pressure equation and its associated contribution to the energy flux,
\begin{align*}
P = fcn(\epsilon_{int}) = (\gamma-1) \epsilon \\
E_{tot} = \frac{\rho \vec{v} \cdot \vec{v}}{2} + \epsilon_{int}
\end{align*}

The secondary equations for gravity,
\begin{align*}
\textcolor{orange}{\nabla^2 \phi_{self} = 4 \pi G \rho} \\
\textcolor{orange}{\phi_g = \phi_{self} + \phi_{static} + \sum_{i} \frac{-G M_i}{|x - x_i|} }
\end{align*}
where $M_i$ is a (discrete) compact point mass at position $x_i$.
Self gravity, stationary potential, and compact objects are enableable separately.
Activating any of them requires choosing a gravitational coupling constant G.

The secondary equations for fluid drag are defined as a dimensionless coefficient $C_d$ times the 
ram force on a spherical particle of area $\sigma$, $\frac{1}{2} \rho v_{rel}^2 \sigma$.
The drag coefficient is a function of three other dimensionless measures, the Reynolds, Knudsen number
and Mach. The equation given below provides accurate drag computation for any Knudsen number,
any Reynolds number below roughly 250000 and any Mach below roughly 0.25, and are asymptotically
exact for $Kn \rightarrow 0$ and $Re \rightarrow 0$ (Stokes flow) and for $Kn \rightarrow \infty$ (Epstein flow).
Accuracy loss will be incurred if gas compressibility is relevant ($M > \approx 0.25)$ or for large $Re$
at which the drag curve cannot realistically be modelled by a single equation.

The calculation of $Re$ requires the viscosity $\nu$ and calculation of the Knudsen number requires the
molecular mean free path $\lambda_{mfp}$ which requires the gas particle scattering cross section.
This in turn requires a model for the gas' fine thermodynamic details and parameters. These are stored 
by Imogen in the \begin{tt}fluidDetailModel.m\end{tt} file which contains a variety of relevant parameter
sets selected by names.

The adopted four parameter formula for soft spheres is appropriate for small gaseous molecules over a wide
range of temperatures. In the limit of hard spherical particles, the temperature dependence of viscosity
reduces to $\xi=1/2$ and of cross section reduces to $\theta=0$. Imogen's tables of molecular cross sections
are derived by requiring that they agree with published values of mean free paths.
For details, the reader is referred to the
classic monograph of Chapman and Enskog on gas kinetic theory, ch. 12.
\begin{align*}
\nu(T) = \nu_0 (T/T_0)^\xi \\
Re = \rho_g dv \sqrt{\sigma_d / \pi} / \nu(T) \\
\lambda_{mfp} = \frac{\mu_g}{\sigma_g \sqrt{2}} (T/T_0)^\theta \frac{1}{\rho_g} \\
Kn = \sqrt{\frac{4 \pi}{\sigma_d}} \lambda_{mfp} \\
C_d = \frac{\left[ 24 / Re + 3.6 Re^{-.319} + \frac{.4072 Re}{8710 + Re} \right]}{\left[1 + Kn(1.142 + .558 e^{-.999/Kn} \right] } \\
F_{drag} = C_d (\frac{1}{2}\rho_g dv^2)\frac{\rho_d}{\mu_d}
\end{align*}
The numerator in the drag coefficient equation is an empirical fit to the well known `standard drag curve'
for spherical particles in infinite fluid. The first term of the numerator is the Stokes coefficient, which is the
only one derivable analytically. The second empirical term is often given as ``$4 Re^{1/3}$''. The given coefficients
and the third term, as well as the denominator (called the Cunningham Coefficient), are given in Allen \& Raabe 1985,
who performed an extremely detailed analysis of hundreds of measurements, accounting for numerous systematic shifts 
and providing the above named coefficients via a numeric residual minimization.

The equation for $F_{drag}$ then computes the force on a single dust particle and multiplies by the dust particle
number density.

Activating radiative emission requires a dimensionful radiation scaling constant $\Gamma_0$.
The code computes as written, using the square of \textit{mass density}. When units are rescaled
the result is shuffled off onto $\Gamma_0$ with the result that
\begin{equation}
\Gamma' = \Gamma_0 \mu^{\theta-2} K_b^{-\theta}
\end{equation}

Imogen contains the following unitful objects which must be defined if the relevant
physics are in use.
\begin{itemize}
\item G - Gravitational coupling constant
\item $K_b$ - Boltzmann constant
\item $\mu$ - Gas or particle mean mass
\item $\sigma$ - Gas kinetic cross section or dust particle cross sectio
\end{itemize}
All initializers inherit the function 

There are four fundamental unitful quantities in Imogen: mass, length, time, and temperature.

As an example of how they might be chosen: Kojima disks are defined with G=1 and M* = 1. The
orbital speed at a radius of 1 is defined as 1. By using $p = \rho^\gamma$ as the equation
of state, implicitly $k_b/\mu$ is chosen.



\subsection{CFD}

At its core Imogen solves the Euler equations, written here in conservative form:
\begin{align*}
\frac{\partial \rho}{\partial t} &+ \frac{\partial}{\partial x_i} \rho v_i &= 0 \\
\frac{\partial (\rho v_i)}{\partial t} &+ \frac{\partial}{\partial x_j} (T_{ij} + \mathbb{I} P) &= 0 \\
\frac{\partial E}{\partial t} &+ \frac{\partial}{\partial x_i} v_i (E + P) &= 0
\end{align*}
with mass density $\rho$, fluid bulk velocity $v_i$, momentum stress tensor
$T_{ij} = \rho v_i v_j$, identity tensor $\mathbb{I}$, scalar thermal pressure $P$,
and total energy density $E = \frac{1}{2} \rho v^2 + \epsilon$.

These are closed by the equation of state $P(\epsilon)$. Imogen implements the
adiabatic ideal gas equation of state, $P = (\gamma-1)\epsilon$ for $\gamma > 1$
(physical gas values are $1 < \gamma \le 5/3$).

With the assumption of a gamma law, the energy flux is written into the
code as $v_i \times (\frac{1}{2} \rho v^2 + \frac{\gamma}{\gamma-1}P)$ with $P$ calculated
before fluxing.

\subsubsection{Operator splitting}

Imogen uses a dimensionally split flux solving scheme. Assuming a splittable Hamiltonian
exists in the form
\[\mathbf{H} = \mathbf{E} + \mathbf{F} + \mathbf{G}\]
where in our case, $E$, $F$, and $G$ represent the fluid fluxes in 3 non-degenerate
directions (normally taken as 3 orthogonal directions in a curvilinear coordinate
system: Here, fluxes in the x, y, and z directions), an approximate scheme of the form
\[e^{\epsilon \mathbf{H}} = e^{\epsilon \mathbf{E}} e^{\epsilon \mathbf{F}}
e^{\epsilon \mathbf{G}} + \mathbb{O}(\epsilon^2) \]
always exists, and the reader may easily verify that the local error associated with
doing this with two operators equals their commutator times $\epsilon^2$.

Strang [reference!] showed that by performing such a first order step, then
applying the same operators in exactly reversed order, will cancel all the first-order
errors. This Strang splitting provides an easy way of constructing second spatial order
solvers.

Denoting the operators that solve the inviscid flux equations in the three directions as
$\mathbf{X}$, $\mathbf{Y}$ and $\mathbf{Z}$, there are 6 ways to order them which are the
elements of the permutation group of $\{ \mathbf{X}, \mathbf{Y}, \mathbf{Z} \}$.
In a two-dimensional simulation, there are only two orderings ($XYYX$ and $YXXY$).

Imogen cycles through them iteration to iteration, the idea being that this will average
away (or at least partly so) any systematic errors associated with a given ordering.

There are additional operators associated with MHD (magnetic fluxing operators), non-inertial
frames, radiation, and scalar potental fields (fixed potentials, point objects or
self-gravitation).

\textbf{The MHD algorithm has major problems with low-beta plasma and seems to be
subtly broken at present.}

\subsection{MHD}

Imogen can evolve magnetic fields in the limit of Ideal MHD where the Euler equations
become
\begin{align*}
\frac{\partial \rho}{\partial t} &= -\frac{\partial}{\partial x_i} \rho v_i \\
\frac{\partial (\rho v_i)}{\partial t} &= -\frac{\partial}{\partial x_j} (T_{ij} + \mathbb{I}(P+B^2/2) \\
\frac{\partial E}{\partial t} &= -\frac{\partial}{\partial x_i} v_i (E + P) - B_i (v_j B_j) \\
\frac{\partial B_i}{\partial t} &= \frac{\partial}{\partial x_i} (v_i B_j - B_i v_j)
\end{align*}
now with the stress tensor $T_{ij} = \rho v_i v_j - B_i B_j$ and under the requirement
$\nabla \mathbf{B} = 0$.

There is no computational requirement that the \textbf{B} field be initialized this way,
however Imogen solves no advection equation for $\mathbf{B}$-charge and any divergence
introduced will simply stay there.

\subsection{Frame rotation}

In many cases a simulation is desired of an object that exhibits coherent rotation at
high speeds.

Imogen has the Eulerian explicit CFL constraint is set by
\[ dt < dx (|v_i| + c_s) \]
and so if $v_i$ can be significantly reduced by setting the grid to co-rotate with a flow,
the timestep can be significantly increased for advection-dominated flows.

Frame rotation introduces the following additional source terms:
\begin{align*}
\partial_t \rho &= 0 \\
\partial_t v &= 2 \omega_z \times \vec{r} + \hat{r} \omega^2 \\
\partial_t \epsilon_{int} &= 0
\end{align*}
i.e. the coriolis and centripetal terms are implemented as fictitous forces.

Imogen implements this term so as to exactly preserve internal energy (the total
energy density is updated as exactly the change in KE density)

Imogen checks for a rotating frame at simulation initialization and performs the
transformation to the noninertial frame for you - \textbf{simulations should be initialized
in the lab inertial frame.}

\subsection{Radiation}

Imogen can solve optically thin radiation with power law cooling,

(power law lambda)

Programming in different cooling laws is extremely simple as a test by making additional
entries to the experiment/Radiation.m file, templated after the thin power law radiation
one. Of course, don't expect them to be as fast as the GPU accelerated routine.

While the power law cooling is reliable enough in and of itself, it has exhibited a
remarkable ability to derange Imogen's fluid solver routines. None of them has exhibited
the ability to handle an extremely slowly-moving radiating shock front (i.e. the motion
associated with the finite accuracy of the simulation) without some measure of oscillatory
density error at the adiabatic jump.

This is related to a generic problem associated with nearly stationary shocks in 
shock-capturing schemes. Without radiation, HLL will exhibit bounded oscillation of the
measured shock position of a plane shock vs the exact result. However radiation adds
an additional feedback loop which greatly increases the ``desire'' of the shock to
place itself exactly at a cell boundary.

(Post pics of evil radiative shocks here)

Work by Roe (REF) has indicated a solution to the instability problem in 1D,
however the extension to multiple dimensions is as yet unclear except perhaps conceptually.

\subsection{Gravity}

\subsubsection{Stationary gravity field}

An arbitrary stationary potential field may be established. This may e.g. represent
a fixed star, or may be used in a Cowling approximation to examine NSG modes of a
self-gravitating equilibrium (NOTE: Still can't solve gravity to generate the equilibrium...)

\subsubsection{Point gravity}

Imogen allows the addition of massive points to the fluid simulation which obey Newton's
laws. Both point-point and point-fluid gravitational force are computed.

These points, ``compact objects'' as Imogen calls them, are used as standins for stars
or planets which cannot be resolved at reasonably available levels of resolution.

Far away from the object, they behave as point masses. Within a distance defined as
its 'radius', matter is considered as having been accreted. Each step, the mass and
linear and angular momentum in cells whose center is within r of a compact mass
are added to the compact object and replaced with grid's vaccuum values instead.

NOTE: This is not the desired behavior in some cases - e.g. a planet would require a surface
pressure, not a surface vaccuum, in order to be embedded in the disk.

\textbf{Point does not feel fluid gravity presently.}

\subsubsection{Self gravity}

This is basically dead for now, especially in parallel.

The serial solver could easily enough be resurrected but there's no way I'm going
to have time to setup a parallel Poisson solver, let alone shovel it onto the GPU.

\section{Physics solvers in the initializers}

Many fluid and physics subproblems come up when writing initial condition generators
for Imogen. Some of these have convenient self-contained solvers available within
Imogen.

The available units include:
\begin{itemize}
\item \begin{tt}J = HDJumpSolver(Mach, $\gamma$, $\theta$)\end{tt}: Returns a structure $J$ describing the
only physical shock solution to a hydrodynamic shock of strength $Mach$ in a fluid of
adiabatic index $\gamma$ and preshock shock-normal flow inclination $\theta$ (in degrees).
\item \begin{tt}J = MHDJumpSolver(Mach, Alfven Mach, $\gamma$, $\theta$)\end{tt}: Solves the MHD jump
equations for the slowest shock permitted.
\item \begin{tt}F = fim(X, @f(X))\end{tt}: Short for Fourier Integral Method, calculates the integral of
anonymous function f on the presumed regularly spaced points $X$ by calculating a polynomial
that renders $f$ 3 times continuous at the endpoints, integrating the polynomial directly
and the residual by Fourier transform.
\item \begin{tt}The ICM\end{tt}: \begin{tt}icm.m\end{tt} is currently very much a prototype and not ready for normal use.
It is short for Integral Constraints Method.
\item \begin{tt}t = pade(c\_n, a, b, x)\end{tt}: Given a polynomial $\sum_n c_n x^n$, calculates the (a,b)
Pade approximant P(a)/Q(b) of the polynomial and evaluates it at x, for any $a+b \le 6$.
\item RadiatingFlowSolver class: Solves the equations of a power-law-radiating hydrodynamic
or magnetohydrodynamic flow. It initializes itself using the 4th (MHD) or 6th (HD) order power
series solution of the flow and then integrates using a 6th order implicit LMM, restarting with
reduced step size when it encounters sharp flow features (knees).
\item Linear wave eigenvectors: The function eigenvectorEntropy, eigenvectorAlfven, eigenvectorSonic,
eigenevectorMA(rho, csq, v, b, k, wavetype) return the linear wave eigenvectors associated with the
flows with the given uniform density, thermal acoustic soundspeed, velocity, magnetic field and wavevector.
Alfven and sonic waves have wavetype +1 for a wave which advances towards $\mathbf{k}$ and -1 for
against $\mathbf{k}$. The MA wavetype can have either sign, with magnitude 2 for a fast MS
wave and magnitude 1 for a slow MS wave. If a nonzero B is passed to a sonic wave, it will invoke the
fast MS solver (as the fast wave is the one connected with the nonmagnetized sonic wave).
\item The bow shock problem, in two-dimensional testing of a source with outflow, has acquired a routine
analyticalOutflow() which solves the exact adiabatic expansion of a $\gamma=5/3$ gas away from a cylinder
with the inner boundary condition of $v_{radial} = v_0$ at $r=r_0$.
\end{itemize}

\section{Programming Experiments}

\subsection{Common parameters of all experiments}

All experiment initializers should inherit from the \begin{tt}Initializer\end{tt} class
and share its not inconsiderable breadth of parameters. Among the more important 
names are
\begin{itemize}
\item \begin{tt}cfl\end{tt} coefficient by which the CFL-limit timestep is scaled. \textbf{WARNING
IMOGEN DOES NOT SANITY CHECK THIS}
\item \begin{tt}gamma\end{tt} - The adiabatic index of the fluid
\item \begin{tt}grid\end{tt} - The [nx ny nz] grid resolution of the \textit{global} domain. This
should normally be set when creating the initializer and not messed with again.
\item \begin{tt}iterMax\end{tt} - the maximum number of timesteps to take
\item \begin{tt}useInSituAnalysis\end{tt} - If nonzero, Imogen uses the given in-situ analysis tool
\item \begin{tt}inSituHandle\end{tt} - @AnalyzerClass which must meet the requirements of an
in-situ analyzer (see REF)
\item \begin{tt}stepsPerInSitu\end{tt} - Number of timesteps between calls to the analyzer
\item \begin{tt}frameRotateOmega\end{tt} - If nonzero, Imogen places the simulation into a rotating
frame and applies appropriate source terms. \textbf{Simulation is still initialized in lab frame}
\item \begin{tt}frameRotateCenter\end{tt} - If frame is rotating, this is the center (\textbf{in cells})
about which the frame rotates on the $(x,y)$ plane.
\end{itemize}

\subsection{How to setup frame rotation}

To turn on the rotating frame during start, simply set a nonzero value for the \begin{tt}frameRotateOmega\end{tt}
parameter, and assign a 2-element array $\left[x_0, y_0\right]$ to \begin{tt}frameRotateCenter\end{tt}
and Imogen will automatically transform the fluid momentum field into the noninertial frame at start.

Caution: $x_0$ and $y_0$ need to be cell indices giving the origin.

If the simulation is in cylindrical coordinates, only rotation about the origin axis is supported.
Any rotation origin other than \begin{tt}[0, 0]\end{tt} will cause a warning.

The rotation rate can be altered during simulation time by calling
\begin{tt}
alterFrameRotation(run, mass,\newline
ener, mom, newOmega)
\end{tt}
in the user's in-situ analysis function. This has no physical effect (outside truncation error)
and will not e.g. generate the Euler term, it simply alters the rate at which the grid traverses
the fluid in the lab frame. Note also that the alterFrameRotation function is not GPU accelerated
and should thus be called rarely.

It is not acceptable to simply alter \begin{tt}run.frameRotateOmega\end{tt} during the run.
This will alter the behavior of the sourcing function, however it will NOT update the
\textit{frame-dependent} momentum and kinetic energy densities.

\subsection{How to setup radiation}

\subsection{How to setup a potential field}

\subsection{How to set up a compact object}

\subsection{How to setup in-situ analysis}

Recognizing that it is not practical to save the entire simulation output for post-hoc analysis in
most cases, Imogen allows the construction of in-situ analyzers. These are classes that must have
two public members:
\begin{itemize}
\item \begin{tt}.FrameAnalyzer(mass, ener, mom, mag, run)\end{tt}
\item \begin{tt}.finish(run)\end{tt}
\end{itemize}

They must be set at initialization time with these three parameters in the initializer:
\begin{itemize}
\item \begin{tt}useInSituAnalysis\end{tt} - Set to nonzero to enable
\item \begin{tt}inSituHandle\end{tt} - @Class which must meet the requirements of an
in-situ analyzer (see above)
\item \begin{tt}stepsPerInSitu\end{tt} - Number of timesteps between calls to the .FrameAnalyzer() function
\end{itemize}

Imogen calls the \begin{tt}.finish\end{tt} method after exiting the CFD iteration loop. At this point the
ISA needs to write to disk any data not already written because \begin{tt}imogen()\end{tt} is about to 
return.

\section{The RealtimePlotter}

The RealtimePlotter is a peripheral that does just what its name suggests, and throws up a pretty little
GUI window to control the plotting process.



\section{Experiments}

\input{../experiment/Advection/advectionDoc.tex}
\input{../experiment/BowShock/bowDoc.tex}
\input{../experiment/CentrifugeTest/centrifugeDoc.tex}
\input{../experiment/DoubleBlast/doubleblastDoc.tex}
\input{../experiment/RiemannProblem/einfeldtDoc.tex}
\input{../experiment/HachisuDisk/hachisuDoc.tex}
\input{../experiment/Implosion/implosionDoc.tex}
\input{../experiment/Jet/jetDoc.tex}
\input{../experiment/KelvinHelmholtz/khDoc.tex}
\input{../experiment/RadiatingShock/radiativeShockDoc.tex}
\input{../experiment/RayleighTaylor/RT_Doc.tex}
\input{../experiment/RichtmyerMeshkov/richtmyermeshkovDoc.tex}
\input{../experiment/SedovTaylorBlastWave/sedovDoc.tex}
\input{../experiment/RiemannProblem/sodDoc.tex}
\input{../experiment/ShuOsherTube/shuoshertubeDoc.tex}

\end{document}
